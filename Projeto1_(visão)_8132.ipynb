{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "n_aP3IIkWBfa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d prithwirajmitra/covid-face-mask-detection-dataset\n",
        "!unzip covid-face-mask-detection-dataset.zip\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZR9NzQwaWO0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import platform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing, metrics, model_selection\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models, datasets, utils\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "HkR5T3n9W29y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "pFc0EJGYXH2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')"
      ],
      "metadata": {
        "id": "Q_j6flYHXmsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms_train = transforms.Compose([\n",
        "                                            transforms.RandomResizedCrop(224),\n",
        "                                            transforms.RandomHorizontalFlip(),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                          ])\n",
        "\n",
        "data_transforms_test = transforms.Compose([\n",
        "                                            transforms.Resize(256),\n",
        "                                            transforms.CenterCrop(224),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                         ])\n"
      ],
      "metadata": {
        "id": "ArBMkxF6XyPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/New Masks Dataset'\n",
        "train_dir = os.path.join(data_dir, 'Train')\n",
        "val_dir = os.path.join(data_dir, 'Validation')\n",
        "test_dir = os.path.join(data_dir, 'Test')\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transform=data_transforms_train)\n",
        "val_dataset = ImageFolder(val_dir, transform=data_transforms_test)\n",
        "test_dataset = ImageFolder(test_dir, transform=data_transforms_test)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "                              train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle = True,\n",
        "                              num_workers=NUM_WORKERS )\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "                            val_dataset,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            shuffle = False,\n",
        "                            num_workers=NUM_WORKERS\n",
        "                           )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "                            test_dataset,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            shuffle = False,\n",
        "                            num_workers=NUM_WORKERS\n",
        "                           )\n",
        "\n",
        "print(\"DataLoaders criados com sucesso usando ImageFolder.\")\n",
        "print(f\"Train batches: {len(train_dataloader)}\")\n",
        "print(f\"Validation batches: {len(val_dataloader)}\")\n",
        "print(f\"Test batches: {len(test_dataloader)}\")"
      ],
      "metadata": {
        "id": "T6eXZgAHYvPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição e Ajuste dos Modelos\n"
      ],
      "metadata": {
        "id": "65O3zanUZngN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "vgg16.classifier[6] = nn.Linear(4096, num_classes)\n",
        "vgg16 = vgg16.to(DEVICE)\n",
        "\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
        "alexnet = alexnet.to(DEVICE)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U7UpKhpnZp5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função de Treino e Validação do Dataset\n"
      ],
      "metadata": {
        "id": "WrDJhYlIZ1TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, model_name=\"Modelo\"):\n",
        "    train_loss_list, val_loss_list = [], []\n",
        "    train_acc_list, val_acc_list = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nÉpoca {epoch+1}/{num_epochs}\")\n",
        "        model.train()\n",
        "        train_loss, train_corrects = 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            train_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = train_loss / len(train_loader.dataset)\n",
        "        epoch_acc = train_corrects.double() / len(train_loader.dataset)\n",
        "        train_loss_list.append(epoch_loss)\n",
        "        train_acc_list.append(epoch_acc.item())\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_corrects = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
        "        val_loss_list.append(val_loss)\n",
        "        val_acc_list.append(val_acc.item())\n",
        "\n",
        "        print(f\"Treino - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n",
        "        print(f\"Validação - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "    epochs_list = list(range(num_epochs))\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(epochs_list, train_loss_list, label=\"Train Loss\", color='magenta')\n",
        "    plt.plot(epochs_list, val_loss_list, label=\"Val Loss\", color='green')\n",
        "    plt.title(f\"Loss por Época - {model_name}\")\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(epochs_list, train_acc_list, label=\"Train Accuracy\", color='magenta')\n",
        "    plt.plot(epochs_list, val_acc_list, label=\"Val Accuracy\", color='green')\n",
        "    plt.title(f\"Acurácia por Época - {model_name}\")\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Acurácia\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Ot6YJjOFZvaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_vgg = optim.Adam(vgg16.parameters(), lr=0.0001)\n",
        "vgg16 = train_model(vgg16, train_dataloader, val_dataloader, criterion, optimizer_vgg,\n",
        "                    num_epochs=50, model_name=\"VGG16\")\n",
        "\n",
        "optimizer_alex = optim.Adam(alexnet.parameters(), lr=0.0001)\n",
        "alexnet = train_model(alexnet, train_dataloader, val_dataloader, criterion, optimizer_alex,\n",
        "                      num_epochs=50, model_name=\"AlexNet\")\n"
      ],
      "metadata": {
        "id": "3ubKLgtCZ6Ll",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avaliação com Matriz de Confusão e as Métricas"
      ],
      "metadata": {
        "id": "Xb3sgG3MaJJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, test_loader, model_name):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=train_dataset.classes,\n",
        "                yticklabels=train_dataset.classes)\n",
        "    plt.title(f\"Matriz de Confusão - {model_name}\")\n",
        "    plt.xlabel(\"Predito\")\n",
        "    plt.ylabel(\"Real\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n--- {model_name} ---\")\n",
        "    print(classification_report(y_true, y_pred, target_names=train_dataset.classes))\n"
      ],
      "metadata": {
        "id": "iQhoFtOzaHDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(vgg16, test_dataloader, \"VGG16\")\n",
        "evaluate_model(alexnet, test_dataloader, \"AlexNet\")\n"
      ],
      "metadata": {
        "id": "h-g23dSSaPmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}